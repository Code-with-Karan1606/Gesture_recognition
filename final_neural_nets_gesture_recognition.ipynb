{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": []
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Gesture Recognition\n",
        "\n",
        "Develop a cool feature in the smart-TV that can recognise five different gestures performed by the user which will help users control the TV without using a remote.\n",
        "\n",
        "The gestures are continuously monitored by the webcam mounted on the TV. Each gesture corresponds to a specific command:\n",
        "\n",
        "1. **Thumbs up:**   Increase the volume\n",
        "2. **Thumbs down:** Decrease the volume\n",
        "3. **Left swipe:**  'Jump' backwards 10 seconds\n",
        "4. **Right swipe:** 'Jump' forward 10 seconds  \n",
        "5. **Stop:**        Pause the movie\n",
        "\n",
        "## Understanding the Dataset\n",
        "1. The training data consists of a few hundred videos categorised into one of the five classes.\n",
        "2. Each video (typically 2-3 seconds long) is divided into a sequence of 30 frames(images).\n",
        "3. These videos have been recorded by various people performing one of the five gestures in front of a webcam - similar to what the smart TV will use. \n",
        "\n",
        "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
      ],
      "metadata": {
        "id": "3Ktt5tC1o1Vb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "#from scipy.misc import imread, imresize\n",
        "import imageio.v2 as imageio\n",
        "from skimage.transform import resize\n",
        "from PIL import Image\n",
        "import datetime\n",
        "import os\n",
        "import random"
      ],
      "metadata": {
        "id": "MqIfpTzpo1Vg",
        "trusted": true
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Code for google.colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q /content/gdrive/MyDrive/Project_data.zip -d /content/Project_data/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXPWMJQGrgos",
        "outputId": "87b023e0-9e2b-4e75-fcd0-6b9957c3ea2c",
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We set the random seed so that the results don't vary drastically."
      ],
      "metadata": {
        "id": "AYIrBLcCo1Vi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(30)\n",
        "import random as rn\n",
        "rn.seed(30)\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "tf.random.set_seed(30)"
      ],
      "metadata": {
        "id": "h7wAOvelo1Vi",
        "trusted": true
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this block, you read the folder names for training and validation."
      ],
      "metadata": {
        "id": "7bU50ZN4o1Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Kaggle\n",
        "#train_doc = np.random.permutation(open('/kaggle/input/rnn-upload/Project_data/train.csv').readlines())\n",
        "#val_doc = np.random.permutation(open('/kaggle/input/rnn-upload/Project_data/val.csv').readlines())\n",
        "\n",
        "#Google collab\n",
        "train_doc = np.random.permutation(open('/content/Project_data/Project_data/train.csv').readlines())\n",
        "val_doc = np.random.permutation(open('/content/Project_data/Project_data/val.csv').readlines())"
      ],
      "metadata": {
        "id": "NzuImr2Ho1Vj",
        "trusted": true
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generator\n",
        "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
      ],
      "metadata": {
        "id": "fMrvZlZpo1Vk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generator(source_path, folder_list, batch_size,img_size,img_idx):\n",
        "    #print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
        "    img_idx = img_idx #create a list of image numbers you want to use for a particular video\n",
        "    while True:\n",
        "        t = np.random.permutation(folder_list)\n",
        "        num_batches =int(len(t)/batch_size) # calculate the number of batches\n",
        "        for batch in range(num_batches): # we iterate over the number of batches\n",
        "            batch_data = np.zeros((batch_size,len(img_idx),img_size,img_size,3)) # x = len(img_idx) is the number of images you use for each video, (y,z) = (img_size,img_size) is the final size of the input images and 3 is the number of channels RGB\n",
        "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
        "            for folder in range(batch_size): # iterate over the batch_size\n",
        "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imageio.imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
        "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
        "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
        "                    image = resize(image, (img_size,img_size))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "                    \n",
        "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
        "        \n",
        "        # write the code for the remaining data points which are left after full batches\n",
        "        leftover_batches = len(t)%batch_size\n",
        "        if leftover_batches != 0:\n",
        "            batch_data = np.zeros((leftover_batches,len(img_idx),img_size, img_size,3)) \n",
        "            # batch_labels is the one hot representation of the output: 10 videos with 5 columns as classes\n",
        "            batch_labels = np.zeros((leftover_batches,5)) \n",
        "            for folder in range(leftover_batches): # iterate over the leftover_batches\n",
        "                imgs = os.listdir(source_path +'/'+t[folder + (num_batches * batch_size)].split(';')[0])\n",
        "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
        "                    image = imageio.imread(source_path +'/'+t[folder + (num_batches * batch_size)].split(';')[0] +'/'+imgs[item]).astype(np.float32)\n",
        "                    image = resize(image, (img_size,img_size))\n",
        "                    batch_data[folder,idx,:,:,0] = (image[:,:,0]) - 104\n",
        "                    batch_data[folder,idx,:,:,1] = (image[:,:,1]) - 117\n",
        "                    batch_data[folder,idx,:,:,2] = (image[:,:,2]) - 123\n",
        "                        \n",
        "                batch_labels[folder, int(t[folder + (num_batches * batch_size)].split(';')[2])] = 1\n",
        "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do"
      ],
      "metadata": {
        "id": "0xGuz-2Yo1Vl",
        "trusted": true
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
      ],
      "metadata": {
        "id": "Y-_XOiWro1Vn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "curr_dt_time = datetime.datetime.now()\n",
        "\n",
        "#Kaggle\n",
        "#train_path = '/kaggle/input/rnn-upload/Project_data/train'\n",
        "#val_path = '/kaggle/input/rnn-upload/Project_data/val'\n",
        "\n",
        "#google collab\n",
        "train_path = '/content/Project_data/Project_data/train'\n",
        "val_path = '/content/Project_data/Project_data/val'\n",
        "\n",
        "num_train_sequences = len(train_doc)\n",
        "print('# training sequences =', num_train_sequences)\n",
        "num_val_sequences = len(val_doc)\n",
        "print('# validation sequences =', num_val_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_GF7E63Lo1Vn",
        "outputId": "dfd0f3e2-026d-4665-815c-35845647237e",
        "trusted": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# training sequences = 663\n",
            "# validation sequences = 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model\n",
        "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
      ],
      "metadata": {
        "id": "vFysL5hfo1Vo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv3D, BatchNormalization, Activation, MaxPooling3D, Dropout, Flatten, Dense\n",
        "from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n",
        "from keras import optimizers\n",
        "\n",
        "# Define the model\n",
        "class Conv3DModel():\n",
        "    def Model3D(self,frames,img_size):\n",
        "      model = Sequential()\n",
        "\n",
        "      model.add(Conv3D(64, (3,3,3), strides=(1,1,1), padding='same', input_shape=(frames,img_size,img_size,3)))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling3D(pool_size=(2,2,1), strides=(2,2,1)))\n",
        "\n",
        "      model.add(Conv3D(128, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "      model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "      model.add(Conv3D(256, (3,3,3), strides=(1,1,1), padding='same'))\n",
        "      model.add(BatchNormalization())\n",
        "      model.add(Activation('relu'))\n",
        "      model.add(MaxPooling3D(pool_size=(2,2,2), strides=(2,2,2), padding='same'))\n",
        "\n",
        "      model.add(Flatten())\n",
        "\n",
        "      model.add(Dropout(0.25))\n",
        "      model.add(Dense(512, activation='relu'))\n",
        "      model.add(Dropout(0.25))\n",
        "      model.add(Dense(5, activation='softmax'))\n",
        "      optimiser = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.7, nesterov=True)\n",
        "      model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
        "      return model"
      ],
      "metadata": {
        "id": "EvmRKuwso1Vo",
        "trusted": true
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialising Global vars\n",
        "def global_vars(img_idx,img_size,batch_size,num_epochs):\n",
        "    print(\"Model Features: No of Epochs = {}, batch_size = {},img_size = {}, no of frames = {}\".format(num_epochs,batch_size,img_size,len(img_idx)))\n",
        "    return img_idx,img_size,batch_size,num_epochs"
      ],
      "metadata": {
        "scrolled": true,
        "id": "wMaD3oS_o1Vp",
        "trusted": true
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 1: No of Epochs = 10 , batch_size = 12 ,shape = (120,120) , no of frames = 10\n"
      ],
      "metadata": {
        "id": "9xV3rRhrDVlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,3,6,9,12,15,18,21,24,27],120,12,10)\n",
        "conv_model1=Conv3DModel()\n",
        "conv_model1=conv_model1.Model3D(len(img_idx),img_size)\n",
        "conv_model1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dghEg_DveTqQ",
        "outputId": "5ae04e21-bd89-4d0c-a7af-3e527f666ac1",
        "trusted": true
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 10, batch_size = 12,img_size = 120, no of frames = 10\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d (Conv3D)             (None, 10, 120, 120, 64)  5248      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 10, 120, 120, 64)  256      \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 10, 120, 120, 64)  0         \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 5, 60, 120, 64)   0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 5, 60, 120, 128)   221312    \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 5, 60, 120, 128)  512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 5, 60, 120, 128)   0         \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 3, 30, 60, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 3, 30, 60, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_2 (Batc  (None, 3, 30, 60, 256)   1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_2 (Activation)   (None, 3, 30, 60, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_2 (MaxPooling  (None, 2, 15, 30, 256)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_3 (Conv3D)           (None, 2, 15, 30, 256)    1769728   \n",
            "                                                                 \n",
            " batch_normalization_3 (Batc  (None, 2, 15, 30, 256)   1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 2, 15, 30, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_3 (MaxPooling  (None, 1, 8, 15, 256)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 30720)             0         \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 30720)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               15729152  \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 18,615,813\n",
            "Trainable params: 18,614,405\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
      ],
      "metadata": {
        "id": "PiR97m0zo1Vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)"
      ],
      "metadata": {
        "id": "exKj8vxlo1Vq",
        "trusted": true
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'model_init' + '_' + str(curr_dt_time).replace(' ','').replace(':','_') + '/'\n",
        "    \n",
        "if not os.path.exists(model_name):\n",
        "    os.mkdir(model_name)\n",
        "        \n",
        "filepath = model_name + 'model-{epoch:05d}-{loss:.5f}-{categorical_accuracy:.5f}-{val_loss:.5f}-{val_categorical_accuracy:.5f}.h5'\n",
        "\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=False, save_weights_only=False, mode='auto', save_freq='epoch')\n",
        "\n",
        "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, verbose=1, mode='min', min_delta=1e-3, cooldown=0, min_lr=0.00001)\n",
        "callbacks_list = [checkpoint, LR]"
      ],
      "metadata": {
        "id": "8odE4EBIo1Vq",
        "trusted": true
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
      ],
      "metadata": {
        "id": "wQ30hCJjo1Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS_OEjNHo1Vr",
        "outputId": "290c0ab8-cab6-4ac7-9d00-f51125e047f6",
        "trusted": true
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now fit the model. This will start training the model and with the help of the checkpoints, you'll be able to save the model at the end of each epoch."
      ],
      "metadata": {
        "id": "4ewNS6Ino1Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model1.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "riCN5pVxo1Vr",
        "outputId": "23c2f93d-7e0a-492d-e0b9-51dc235d2cfe",
        "trusted": true
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 2.2269 - categorical_accuracy: 0.4027\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-2.22694-0.40271-2.43872-0.23000.h5\n",
            "56/56 [==============================] - 86s 1s/step - loss: 2.2269 - categorical_accuracy: 0.4027 - val_loss: 2.4387 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 2/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 1.0933 - categorical_accuracy: 0.5641\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-1.09330-0.56410-1.49417-0.40000.h5\n",
            "56/56 [==============================] - 69s 1s/step - loss: 1.0933 - categorical_accuracy: 0.5641 - val_loss: 1.4942 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 3/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.8459 - categorical_accuracy: 0.6440\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-0.84587-0.64404-1.00986-0.61000.h5\n",
            "56/56 [==============================] - 67s 1s/step - loss: 0.8459 - categorical_accuracy: 0.6440 - val_loss: 1.0099 - val_categorical_accuracy: 0.6100 - lr: 0.0010\n",
            "Epoch 4/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.7478 - categorical_accuracy: 0.7119\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-0.74777-0.71192-0.77149-0.71000.h5\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.7478 - categorical_accuracy: 0.7119 - val_loss: 0.7715 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
            "Epoch 5/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.5672 - categorical_accuracy: 0.7949\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-0.56720-0.79487-1.54053-0.38000.h5\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.5672 - categorical_accuracy: 0.7949 - val_loss: 1.5405 - val_categorical_accuracy: 0.3800 - lr: 0.0010\n",
            "Epoch 6/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.5769 - categorical_accuracy: 0.7813\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-0.57693-0.78130-0.77895-0.72000.h5\n",
            "\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.5769 - categorical_accuracy: 0.7813 - val_loss: 0.7790 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 7/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.3727 - categorical_accuracy: 0.8612\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-0.37268-0.86124-0.67472-0.77000.h5\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.3727 - categorical_accuracy: 0.8612 - val_loss: 0.6747 - val_categorical_accuracy: 0.7700 - lr: 5.0000e-04\n",
            "Epoch 8/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.2803 - categorical_accuracy: 0.9020\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-0.28028-0.90196-0.61514-0.78000.h5\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.2803 - categorical_accuracy: 0.9020 - val_loss: 0.6151 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
            "Epoch 9/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.2444 - categorical_accuracy: 0.9140\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-0.24444-0.91403-0.90731-0.68000.h5\n",
            "56/56 [==============================] - 68s 1s/step - loss: 0.2444 - categorical_accuracy: 0.9140 - val_loss: 0.9073 - val_categorical_accuracy: 0.6800 - lr: 5.0000e-04\n",
            "Epoch 10/10\n",
            "56/56 [==============================] - ETA: 0s - loss: 0.2482 - categorical_accuracy: 0.8989\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-0.24825-0.89894-0.57587-0.82000.h5\n",
            "56/56 [==============================] - 69s 1s/step - loss: 0.2482 - categorical_accuracy: 0.8989 - val_loss: 0.5759 - val_categorical_accuracy: 0.8200 - lr: 5.0000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f20d0112a00>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 2: No of Epochs = 15 , batch_size = 16 ,shape = (100,100) , no of frames = 12\n"
      ],
      "metadata": {
        "id": "9fEGUgyzDVlF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,3,6,9,11,13,15,18,20,22,24,27],100,16,15)\n",
        "conv_model2=Conv3DModel()\n",
        "conv_model2=conv_model2.Model3D(len(img_idx),img_size)\n",
        "conv_model2.summary()"
      ],
      "metadata": {
        "id": "QnWh20lAo1Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fffa1fb5-75c3-42b4-91d0-4d13da4f239a",
        "trusted": true
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 15, batch_size = 16,img_size = 100, no of frames = 12\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_4 (Conv3D)           (None, 12, 100, 100, 64)  5248      \n",
            "                                                                 \n",
            " batch_normalization_4 (Batc  (None, 12, 100, 100, 64)  256      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_4 (Activation)   (None, 12, 100, 100, 64)  0         \n",
            "                                                                 \n",
            " max_pooling3d_4 (MaxPooling  (None, 6, 50, 100, 64)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_5 (Conv3D)           (None, 6, 50, 100, 128)   221312    \n",
            "                                                                 \n",
            " batch_normalization_5 (Batc  (None, 6, 50, 100, 128)  512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_5 (Activation)   (None, 6, 50, 100, 128)   0         \n",
            "                                                                 \n",
            " max_pooling3d_5 (MaxPooling  (None, 3, 25, 50, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_6 (Conv3D)           (None, 3, 25, 50, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_6 (Batc  (None, 3, 25, 50, 256)   1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_6 (Activation)   (None, 3, 25, 50, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_6 (MaxPooling  (None, 2, 13, 25, 256)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_7 (Conv3D)           (None, 2, 13, 25, 256)    1769728   \n",
            "                                                                 \n",
            " batch_normalization_7 (Batc  (None, 2, 13, 25, 256)   1024      \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_7 (Activation)   (None, 2, 13, 25, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_7 (MaxPooling  (None, 1, 7, 13, 256)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 23296)             0         \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 23296)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               11928064  \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,814,725\n",
            "Trainable params: 14,813,317\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YrEk1Mui3zW",
        "outputId": "8679d790-67d4-4b96-9382-ea897c79983b",
        "trusted": true
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "42\n",
            "7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model2.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1fPQ8Zs8Uem",
        "outputId": "3cd96645-c36c-4a61-e36d-f03dbe22ad2b",
        "trusted": true
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 2.2014 - categorical_accuracy: 0.3846\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-2.20137-0.38462-4.06161-0.23000.h5\n",
            "42/42 [==============================] - 79s 2s/step - loss: 2.2014 - categorical_accuracy: 0.3846 - val_loss: 4.0616 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 2/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.9858 - categorical_accuracy: 0.6063\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-0.98578-0.60633-3.49719-0.24000.h5\n",
            "42/42 [==============================] - 79s 2s/step - loss: 0.9858 - categorical_accuracy: 0.6063 - val_loss: 3.4972 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
            "Epoch 3/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.8683 - categorical_accuracy: 0.6652\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-0.86834-0.66516-0.91775-0.67000.h5\n",
            "42/42 [==============================] - 79s 2s/step - loss: 0.8683 - categorical_accuracy: 0.6652 - val_loss: 0.9177 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 4/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.6669 - categorical_accuracy: 0.7632\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-0.66693-0.76320-1.34463-0.48000.h5\n",
            "42/42 [==============================] - 79s 2s/step - loss: 0.6669 - categorical_accuracy: 0.7632 - val_loss: 1.3446 - val_categorical_accuracy: 0.4800 - lr: 0.0010\n",
            "Epoch 5/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.5758 - categorical_accuracy: 0.7813\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-0.57577-0.78130-0.73670-0.68000.h5\n",
            "42/42 [==============================] - 79s 2s/step - loss: 0.5758 - categorical_accuracy: 0.7813 - val_loss: 0.7367 - val_categorical_accuracy: 0.6800 - lr: 0.0010\n",
            "Epoch 6/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.5071 - categorical_accuracy: 0.8069\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-0.50713-0.80694-0.65326-0.79000.h5\n",
            "42/42 [==============================] - 71s 2s/step - loss: 0.5071 - categorical_accuracy: 0.8069 - val_loss: 0.6533 - val_categorical_accuracy: 0.7900 - lr: 0.0010\n",
            "Epoch 7/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.3580 - categorical_accuracy: 0.8733\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-0.35797-0.87330-0.81745-0.72000.h5\n",
            "42/42 [==============================] - 69s 2s/step - loss: 0.3580 - categorical_accuracy: 0.8733 - val_loss: 0.8175 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 8/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.2865 - categorical_accuracy: 0.9035\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-0.28651-0.90347-0.70239-0.74000.h5\n",
            "\n",
            "Epoch 8: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "42/42 [==============================] - 70s 2s/step - loss: 0.2865 - categorical_accuracy: 0.9035 - val_loss: 0.7024 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
            "Epoch 9/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.2192 - categorical_accuracy: 0.9336\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-0.21918-0.93363-0.55846-0.80000.h5\n",
            "42/42 [==============================] - 80s 2s/step - loss: 0.2192 - categorical_accuracy: 0.9336 - val_loss: 0.5585 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
            "Epoch 10/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1714 - categorical_accuracy: 0.9517\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-0.17144-0.95173-0.60946-0.78000.h5\n",
            "42/42 [==============================] - 81s 2s/step - loss: 0.1714 - categorical_accuracy: 0.9517 - val_loss: 0.6095 - val_categorical_accuracy: 0.7800 - lr: 5.0000e-04\n",
            "Epoch 11/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1445 - categorical_accuracy: 0.9668\n",
            "Epoch 11: saving model to model_init_2023-04-1716_33_09.254485/model-00011-0.14450-0.96682-0.40137-0.86000.h5\n",
            "42/42 [==============================] - 70s 2s/step - loss: 0.1445 - categorical_accuracy: 0.9668 - val_loss: 0.4014 - val_categorical_accuracy: 0.8600 - lr: 5.0000e-04\n",
            "Epoch 12/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1409 - categorical_accuracy: 0.9578\n",
            "Epoch 12: saving model to model_init_2023-04-1716_33_09.254485/model-00012-0.14090-0.95777-0.56585-0.80000.h5\n",
            "42/42 [==============================] - 70s 2s/step - loss: 0.1409 - categorical_accuracy: 0.9578 - val_loss: 0.5658 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
            "Epoch 13/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.1170 - categorical_accuracy: 0.9729\n",
            "Epoch 13: saving model to model_init_2023-04-1716_33_09.254485/model-00013-0.11701-0.97285-0.44365-0.87000.h5\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "42/42 [==============================] - 71s 2s/step - loss: 0.1170 - categorical_accuracy: 0.9729 - val_loss: 0.4437 - val_categorical_accuracy: 0.8700 - lr: 5.0000e-04\n",
            "Epoch 14/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0936 - categorical_accuracy: 0.9804\n",
            "Epoch 14: saving model to model_init_2023-04-1716_33_09.254485/model-00014-0.09360-0.98039-0.44623-0.87000.h5\n",
            "42/42 [==============================] - 69s 2s/step - loss: 0.0936 - categorical_accuracy: 0.9804 - val_loss: 0.4462 - val_categorical_accuracy: 0.8700 - lr: 2.5000e-04\n",
            "Epoch 15/15\n",
            "42/42 [==============================] - ETA: 0s - loss: 0.0892 - categorical_accuracy: 0.9849\n",
            "Epoch 15: saving model to model_init_2023-04-1716_33_09.254485/model-00015-0.08916-0.98492-0.56175-0.80000.h5\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "42/42 [==============================] - 70s 2s/step - loss: 0.0892 - categorical_accuracy: 0.9849 - val_loss: 0.5617 - val_categorical_accuracy: 0.8000 - lr: 2.5000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f205b3f4130>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 3: No of Epochs = 20 , batch_size = 32 ,shape = (70,70) , no of frames = 12"
      ],
      "metadata": {
        "id": "z0_UuRJ2DVlG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,3,6,9,11,13,15,18,20,22,24,27],70,32,20)\n",
        "conv_model3=Conv3DModel()\n",
        "conv_model3=conv_model3.Model3D(len(img_idx),img_size)\n",
        "conv_model3.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIxpr4AELw1X",
        "outputId": "eacb1b33-2a7c-4d42-fd93-ca607c218809",
        "trusted": true
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 20, batch_size = 32,img_size = 70, no of frames = 12\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_8 (Conv3D)           (None, 12, 70, 70, 64)    5248      \n",
            "                                                                 \n",
            " batch_normalization_8 (Batc  (None, 12, 70, 70, 64)   256       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_8 (Activation)   (None, 12, 70, 70, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_8 (MaxPooling  (None, 6, 35, 70, 64)    0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_9 (Conv3D)           (None, 6, 35, 70, 128)    221312    \n",
            "                                                                 \n",
            " batch_normalization_9 (Batc  (None, 6, 35, 70, 128)   512       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_9 (Activation)   (None, 6, 35, 70, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_9 (MaxPooling  (None, 3, 18, 35, 128)   0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " conv3d_10 (Conv3D)          (None, 3, 18, 35, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_10 (Bat  (None, 3, 18, 35, 256)   1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_10 (Activation)  (None, 3, 18, 35, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_10 (MaxPoolin  (None, 2, 9, 18, 256)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_11 (Conv3D)          (None, 2, 9, 18, 256)     1769728   \n",
            "                                                                 \n",
            " batch_normalization_11 (Bat  (None, 2, 9, 18, 256)    1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_11 (Activation)  (None, 2, 9, 18, 256)     0         \n",
            "                                                                 \n",
            " max_pooling3d_11 (MaxPoolin  (None, 1, 5, 9, 256)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 11520)             0         \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 11520)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 512)               5898752   \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 8,785,413\n",
            "Trainable params: 8,784,005\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KT4UYA57l0zZ",
        "outputId": "774f2b89-d8e2-4d09-8cee-6acf0b7730d9",
        "trusted": true
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model3.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPiQwWKml6yK",
        "outputId": "2b9be8eb-2198-4111-99ec-ce0104bccbae",
        "trusted": true
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.9356 - categorical_accuracy: 0.3891\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-1.93557-0.38914-5.54214-0.23000.h5\n",
            "21/21 [==============================] - 69s 3s/step - loss: 1.9356 - categorical_accuracy: 0.3891 - val_loss: 5.5421 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0892 - categorical_accuracy: 0.5641\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-1.08915-0.56410-3.93925-0.21000.h5\n",
            "21/21 [==============================] - 62s 3s/step - loss: 1.0892 - categorical_accuracy: 0.5641 - val_loss: 3.9393 - val_categorical_accuracy: 0.2100 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8361 - categorical_accuracy: 0.6787\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-0.83607-0.67873-3.97862-0.27000.h5\n",
            "21/21 [==============================] - 72s 4s/step - loss: 0.8361 - categorical_accuracy: 0.6787 - val_loss: 3.9786 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6624 - categorical_accuracy: 0.7496\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-0.66239-0.74962-1.92119-0.27000.h5\n",
            "21/21 [==============================] - 61s 3s/step - loss: 0.6624 - categorical_accuracy: 0.7496 - val_loss: 1.9212 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6131 - categorical_accuracy: 0.7481\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-0.61312-0.74811-1.70335-0.43000.h5\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.6131 - categorical_accuracy: 0.7481 - val_loss: 1.7034 - val_categorical_accuracy: 0.4300 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5144 - categorical_accuracy: 0.8160\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-0.51440-0.81599-1.58371-0.37000.h5\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.5144 - categorical_accuracy: 0.8160 - val_loss: 1.5837 - val_categorical_accuracy: 0.3700 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4277 - categorical_accuracy: 0.8462\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-0.42769-0.84615-0.80547-0.71000.h5\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.4277 - categorical_accuracy: 0.8462 - val_loss: 0.8055 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3334 - categorical_accuracy: 0.8974\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-0.33342-0.89744-0.80026-0.69000.h5\n",
            "21/21 [==============================] - 72s 4s/step - loss: 0.3334 - categorical_accuracy: 0.8974 - val_loss: 0.8003 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3161 - categorical_accuracy: 0.8974\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-0.31610-0.89744-0.76033-0.73000.h5\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.3161 - categorical_accuracy: 0.8974 - val_loss: 0.7603 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2991 - categorical_accuracy: 0.8914\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-0.29911-0.89140-0.75803-0.67000.h5\n",
            "21/21 [==============================] - 72s 4s/step - loss: 0.2991 - categorical_accuracy: 0.8914 - val_loss: 0.7580 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2497 - categorical_accuracy: 0.9186\n",
            "Epoch 11: saving model to model_init_2023-04-1716_33_09.254485/model-00011-0.24971-0.91855-0.68871-0.75000.h5\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.2497 - categorical_accuracy: 0.9186 - val_loss: 0.6887 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2093 - categorical_accuracy: 0.9351\n",
            "Epoch 12: saving model to model_init_2023-04-1716_33_09.254485/model-00012-0.20925-0.93514-0.63471-0.74000.h5\n",
            "21/21 [==============================] - 72s 4s/step - loss: 0.2093 - categorical_accuracy: 0.9351 - val_loss: 0.6347 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1795 - categorical_accuracy: 0.9457\n",
            "Epoch 13: saving model to model_init_2023-04-1716_33_09.254485/model-00013-0.17946-0.94570-0.63448-0.78000.h5\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.1795 - categorical_accuracy: 0.9457 - val_loss: 0.6345 - val_categorical_accuracy: 0.7800 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1299 - categorical_accuracy: 0.9683\n",
            "Epoch 14: saving model to model_init_2023-04-1716_33_09.254485/model-00014-0.12994-0.96833-0.66266-0.76000.h5\n",
            "\n",
            "Epoch 14: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.1299 - categorical_accuracy: 0.9683 - val_loss: 0.6627 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1028 - categorical_accuracy: 0.9683\n",
            "Epoch 15: saving model to model_init_2023-04-1716_33_09.254485/model-00015-0.10284-0.96833-0.68756-0.70000.h5\n",
            "21/21 [==============================] - 70s 3s/step - loss: 0.1028 - categorical_accuracy: 0.9683 - val_loss: 0.6876 - val_categorical_accuracy: 0.7000 - lr: 5.0000e-04\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1065 - categorical_accuracy: 0.9744\n",
            "Epoch 16: saving model to model_init_2023-04-1716_33_09.254485/model-00016-0.10647-0.97436-0.69948-0.76000.h5\n",
            "\n",
            "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "21/21 [==============================] - 71s 4s/step - loss: 0.1065 - categorical_accuracy: 0.9744 - val_loss: 0.6995 - val_categorical_accuracy: 0.7600 - lr: 5.0000e-04\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0839 - categorical_accuracy: 0.9819\n",
            "Epoch 17: saving model to model_init_2023-04-1716_33_09.254485/model-00017-0.08388-0.98190-0.61522-0.79000.h5\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.0839 - categorical_accuracy: 0.9819 - val_loss: 0.6152 - val_categorical_accuracy: 0.7900 - lr: 2.5000e-04\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0892 - categorical_accuracy: 0.9819\n",
            "Epoch 18: saving model to model_init_2023-04-1716_33_09.254485/model-00018-0.08921-0.98190-0.65779-0.76000.h5\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0892 - categorical_accuracy: 0.9819 - val_loss: 0.6578 - val_categorical_accuracy: 0.7600 - lr: 2.5000e-04\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0764 - categorical_accuracy: 0.9879\n",
            "Epoch 19: saving model to model_init_2023-04-1716_33_09.254485/model-00019-0.07642-0.98793-0.56205-0.80000.h5\n",
            "21/21 [==============================] - 64s 3s/step - loss: 0.0764 - categorical_accuracy: 0.9879 - val_loss: 0.5621 - val_categorical_accuracy: 0.8000 - lr: 2.5000e-04\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0698 - categorical_accuracy: 0.9849\n",
            "Epoch 20: saving model to model_init_2023-04-1716_33_09.254485/model-00020-0.06977-0.98492-0.59569-0.82000.h5\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0698 - categorical_accuracy: 0.9849 - val_loss: 0.5957 - val_categorical_accuracy: 0.8200 - lr: 2.5000e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f2103b11250>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 4: No of Epochs = 20 , batch_size = 32 ,shape = (70,70) , no of frames = 18"
      ],
      "metadata": {
        "id": "XNiWKSI4DVlI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,2,4,6,7,8,10,12,14,15,16,17,18,20,22,24,26,28],70,32,20)\n",
        "conv_model4=Conv3DModel()\n",
        "conv_model4=conv_model4.Model3D(len(img_idx),img_size)\n",
        "conv_model4.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyoz71RhuMIB",
        "outputId": "d6b769f3-111e-47a5-a090-839cba3b7201",
        "trusted": true
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 20, batch_size = 32,img_size = 70, no of frames = 18\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_12 (Conv3D)          (None, 18, 70, 70, 64)    5248      \n",
            "                                                                 \n",
            " batch_normalization_12 (Bat  (None, 18, 70, 70, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_12 (Activation)  (None, 18, 70, 70, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_12 (MaxPoolin  (None, 9, 35, 70, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_13 (Conv3D)          (None, 9, 35, 70, 128)    221312    \n",
            "                                                                 \n",
            " batch_normalization_13 (Bat  (None, 9, 35, 70, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_13 (Activation)  (None, 9, 35, 70, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_13 (MaxPoolin  (None, 5, 18, 35, 128)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_14 (Conv3D)          (None, 5, 18, 35, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_14 (Bat  (None, 5, 18, 35, 256)   1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_14 (Activation)  (None, 5, 18, 35, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_14 (MaxPoolin  (None, 3, 9, 18, 256)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_15 (Conv3D)          (None, 3, 9, 18, 256)     1769728   \n",
            "                                                                 \n",
            " batch_normalization_15 (Bat  (None, 3, 9, 18, 256)    1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_15 (Activation)  (None, 3, 9, 18, 256)     0         \n",
            "                                                                 \n",
            " max_pooling3d_15 (MaxPoolin  (None, 2, 5, 9, 256)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 23040)             0         \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 23040)             0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 512)               11796992  \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,683,653\n",
            "Trainable params: 14,682,245\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hqUylyO14jeE",
        "outputId": "6b6fb497-64d6-42de-fccd-99b2ca7c73d0",
        "trusted": true
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21\n",
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model4.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hNhDrVl04jrA",
        "outputId": "0f0d3ed3-c118-4308-f320-3217d9f0a1ca",
        "trusted": true
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 2.5798 - categorical_accuracy: 0.3379\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-2.57979-0.33786-4.08769-0.23000.h5\n",
            "21/21 [==============================] - 117s 5s/step - loss: 2.5798 - categorical_accuracy: 0.3379 - val_loss: 4.0877 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 1.0416 - categorical_accuracy: 0.5867\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-1.04165-0.58673-3.22046-0.24000.h5\n",
            "21/21 [==============================] - 97s 5s/step - loss: 1.0416 - categorical_accuracy: 0.5867 - val_loss: 3.2205 - val_categorical_accuracy: 0.2400 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.8160 - categorical_accuracy: 0.6833\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-0.81599-0.68326-2.88045-0.23000.h5\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.8160 - categorical_accuracy: 0.6833 - val_loss: 2.8805 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.6502 - categorical_accuracy: 0.7436\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-0.65017-0.74359-2.05266-0.25000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.6502 - categorical_accuracy: 0.7436 - val_loss: 2.0527 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.5639 - categorical_accuracy: 0.7994\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-0.56393-0.79940-1.23942-0.44000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.5639 - categorical_accuracy: 0.7994 - val_loss: 1.2394 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.4981 - categorical_accuracy: 0.8175\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-0.49808-0.81750-1.03163-0.55000.h5\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.4981 - categorical_accuracy: 0.8175 - val_loss: 1.0316 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3850 - categorical_accuracy: 0.8658\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-0.38504-0.86576-0.75567-0.72000.h5\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.3850 - categorical_accuracy: 0.8658 - val_loss: 0.7557 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.3308 - categorical_accuracy: 0.8944\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-0.33078-0.89442-0.72377-0.72000.h5\n",
            "21/21 [==============================] - 91s 5s/step - loss: 0.3308 - categorical_accuracy: 0.8944 - val_loss: 0.7238 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2860 - categorical_accuracy: 0.8989\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-0.28604-0.89894-0.66850-0.72000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.2860 - categorical_accuracy: 0.8989 - val_loss: 0.6685 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.2511 - categorical_accuracy: 0.9155\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-0.25111-0.91554-0.69087-0.71000.h5\n",
            "21/21 [==============================] - 94s 5s/step - loss: 0.2511 - categorical_accuracy: 0.9155 - val_loss: 0.6909 - val_categorical_accuracy: 0.7100 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1851 - categorical_accuracy: 0.9457\n",
            "Epoch 11: saving model to model_init_2023-04-1716_33_09.254485/model-00011-0.18506-0.94570-0.54202-0.80000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.1851 - categorical_accuracy: 0.9457 - val_loss: 0.5420 - val_categorical_accuracy: 0.8000 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1579 - categorical_accuracy: 0.9608\n",
            "Epoch 12: saving model to model_init_2023-04-1716_33_09.254485/model-00012-0.15790-0.96078-0.83826-0.69000.h5\n",
            "21/21 [==============================] - 97s 5s/step - loss: 0.1579 - categorical_accuracy: 0.9608 - val_loss: 0.8383 - val_categorical_accuracy: 0.6900 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1372 - categorical_accuracy: 0.9593\n",
            "Epoch 13: saving model to model_init_2023-04-1716_33_09.254485/model-00013-0.13721-0.95928-0.84882-0.67000.h5\n",
            "\n",
            "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.1372 - categorical_accuracy: 0.9593 - val_loss: 0.8488 - val_categorical_accuracy: 0.6700 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.1171 - categorical_accuracy: 0.9789\n",
            "Epoch 14: saving model to model_init_2023-04-1716_33_09.254485/model-00014-0.11707-0.97888-0.78941-0.74000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.1171 - categorical_accuracy: 0.9789 - val_loss: 0.7894 - val_categorical_accuracy: 0.7400 - lr: 5.0000e-04\n",
            "Epoch 15/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0963 - categorical_accuracy: 0.9834\n",
            "Epoch 15: saving model to model_init_2023-04-1716_33_09.254485/model-00015-0.09633-0.98341-0.57418-0.80000.h5\n",
            "\n",
            "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0963 - categorical_accuracy: 0.9834 - val_loss: 0.5742 - val_categorical_accuracy: 0.8000 - lr: 5.0000e-04\n",
            "Epoch 16/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0891 - categorical_accuracy: 0.9834\n",
            "Epoch 16: saving model to model_init_2023-04-1716_33_09.254485/model-00016-0.08905-0.98341-0.66399-0.76000.h5\n",
            "21/21 [==============================] - 92s 5s/step - loss: 0.0891 - categorical_accuracy: 0.9834 - val_loss: 0.6640 - val_categorical_accuracy: 0.7600 - lr: 2.5000e-04\n",
            "Epoch 17/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0792 - categorical_accuracy: 0.9864\n",
            "Epoch 17: saving model to model_init_2023-04-1716_33_09.254485/model-00017-0.07924-0.98643-0.58803-0.82000.h5\n",
            "\n",
            "Epoch 17: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "21/21 [==============================] - 95s 5s/step - loss: 0.0792 - categorical_accuracy: 0.9864 - val_loss: 0.5880 - val_categorical_accuracy: 0.8200 - lr: 2.5000e-04\n",
            "Epoch 18/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0677 - categorical_accuracy: 0.9910\n",
            "Epoch 18: saving model to model_init_2023-04-1716_33_09.254485/model-00018-0.06770-0.99095-0.46368-0.83000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0677 - categorical_accuracy: 0.9910 - val_loss: 0.4637 - val_categorical_accuracy: 0.8300 - lr: 1.2500e-04\n",
            "Epoch 19/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0735 - categorical_accuracy: 0.9910\n",
            "Epoch 19: saving model to model_init_2023-04-1716_33_09.254485/model-00019-0.07351-0.99095-0.50393-0.84000.h5\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0735 - categorical_accuracy: 0.9910 - val_loss: 0.5039 - val_categorical_accuracy: 0.8400 - lr: 1.2500e-04\n",
            "Epoch 20/20\n",
            "21/21 [==============================] - ETA: 0s - loss: 0.0662 - categorical_accuracy: 0.9925\n",
            "Epoch 20: saving model to model_init_2023-04-1716_33_09.254485/model-00020-0.06617-0.99246-0.53878-0.82000.h5\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "21/21 [==============================] - 96s 5s/step - loss: 0.0662 - categorical_accuracy: 0.9925 - val_loss: 0.5388 - val_categorical_accuracy: 0.8200 - lr: 1.2500e-04\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f205ac42910>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 5: No of Epochs = 20 , batch_size = 64 ,shape = (50,50) , no of frames = 12"
      ],
      "metadata": {
        "id": "5lr4SQrnDVlJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,3,6,9,11,13,15,18,20,22,24,27],50,64,20)\n",
        "conv_model5=Conv3DModel()\n",
        "conv_model5=conv_model5.Model3D(len(img_idx),img_size)\n",
        "conv_model5.summary()"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxI-7lznDVlJ",
        "outputId": "da2fd59a-c444-4fae-89ff-8f62df39a23c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 20, batch_size = 64,img_size = 50, no of frames = 12\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv3d_16 (Conv3D)          (None, 12, 50, 50, 64)    5248      \n",
            "                                                                 \n",
            " batch_normalization_16 (Bat  (None, 12, 50, 50, 64)   256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_16 (Activation)  (None, 12, 50, 50, 64)    0         \n",
            "                                                                 \n",
            " max_pooling3d_16 (MaxPoolin  (None, 6, 25, 50, 64)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_17 (Conv3D)          (None, 6, 25, 50, 128)    221312    \n",
            "                                                                 \n",
            " batch_normalization_17 (Bat  (None, 6, 25, 50, 128)   512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_17 (Activation)  (None, 6, 25, 50, 128)    0         \n",
            "                                                                 \n",
            " max_pooling3d_17 (MaxPoolin  (None, 3, 13, 25, 128)   0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_18 (Conv3D)          (None, 3, 13, 25, 256)    884992    \n",
            "                                                                 \n",
            " batch_normalization_18 (Bat  (None, 3, 13, 25, 256)   1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_18 (Activation)  (None, 3, 13, 25, 256)    0         \n",
            "                                                                 \n",
            " max_pooling3d_18 (MaxPoolin  (None, 2, 7, 13, 256)    0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " conv3d_19 (Conv3D)          (None, 2, 7, 13, 256)     1769728   \n",
            "                                                                 \n",
            " batch_normalization_19 (Bat  (None, 2, 7, 13, 256)    1024      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_19 (Activation)  (None, 2, 7, 13, 256)     0         \n",
            "                                                                 \n",
            " max_pooling3d_19 (MaxPoolin  (None, 1, 4, 7, 256)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 7168)              0         \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 7168)              0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 512)               3670528   \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 512)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 5)                 2565      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 6,557,189\n",
            "Trainable params: 6,555,781\n",
            "Non-trainable params: 1,408\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbTKVKakDVlK",
        "outputId": "d0e115dc-46dd-45a2-867d-4ce5a6ffe608"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conv_model5.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maLH7HusDVlK",
        "outputId": "bda71311-f942-4f30-e4d7-21a59f36649b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 2.2296 - categorical_accuracy: 0.3152\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-2.22962-0.31523-3.42792-0.28000.h5\n",
            "11/11 [==============================] - 73s 7s/step - loss: 2.2296 - categorical_accuracy: 0.3152 - val_loss: 3.4279 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2073 - categorical_accuracy: 0.5143\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-1.20733-0.51433-3.41594-0.25000.h5\n",
            "11/11 [==============================] - 70s 7s/step - loss: 1.2073 - categorical_accuracy: 0.5143 - val_loss: 3.4159 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0167 - categorical_accuracy: 0.5792\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-1.01675-0.57919-2.87650-0.23000.h5\n",
            "11/11 [==============================] - 69s 7s/step - loss: 1.0167 - categorical_accuracy: 0.5792 - val_loss: 2.8765 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7181 - categorical_accuracy: 0.7240\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-0.71806-0.72398-2.67016-0.25000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 0.7181 - categorical_accuracy: 0.7240 - val_loss: 2.6702 - val_categorical_accuracy: 0.2500 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7019 - categorical_accuracy: 0.7391\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-0.70192-0.73906-2.70451-0.23000.h5\n",
            "11/11 [==============================] - 70s 7s/step - loss: 0.7019 - categorical_accuracy: 0.7391 - val_loss: 2.7045 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.6582 - categorical_accuracy: 0.7572\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-0.65817-0.75716-2.56340-0.20000.h5\n",
            "11/11 [==============================] - 63s 6s/step - loss: 0.6582 - categorical_accuracy: 0.7572 - val_loss: 2.5634 - val_categorical_accuracy: 0.2000 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5665 - categorical_accuracy: 0.7798\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-0.56651-0.77979-2.02811-0.29000.h5\n",
            "11/11 [==============================] - 67s 7s/step - loss: 0.5665 - categorical_accuracy: 0.7798 - val_loss: 2.0281 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.5020 - categorical_accuracy: 0.8160\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-0.50199-0.81599-1.77577-0.34000.h5\n",
            "11/11 [==============================] - 69s 7s/step - loss: 0.5020 - categorical_accuracy: 0.8160 - val_loss: 1.7758 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.4486 - categorical_accuracy: 0.8431\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-0.44864-0.84314-1.59131-0.34000.h5\n",
            "11/11 [==============================] - 70s 7s/step - loss: 0.4486 - categorical_accuracy: 0.8431 - val_loss: 1.5913 - val_categorical_accuracy: 0.3400 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3556 - categorical_accuracy: 0.8718\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-0.35557-0.87179-1.33320-0.44000.h5\n",
            "11/11 [==============================] - 65s 6s/step - loss: 0.3556 - categorical_accuracy: 0.8718 - val_loss: 1.3332 - val_categorical_accuracy: 0.4400 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3457 - categorical_accuracy: 0.8854\n",
            "Epoch 11: saving model to model_init_2023-04-1716_33_09.254485/model-00011-0.34573-0.88537-0.99182-0.57000.h5\n",
            "11/11 [==============================] - 68s 7s/step - loss: 0.3457 - categorical_accuracy: 0.8854 - val_loss: 0.9918 - val_categorical_accuracy: 0.5700 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2950 - categorical_accuracy: 0.9005\n",
            "Epoch 12: saving model to model_init_2023-04-1716_33_09.254485/model-00012-0.29500-0.90045-0.96092-0.59000.h5\n",
            "11/11 [==============================] - 65s 6s/step - loss: 0.2950 - categorical_accuracy: 0.9005 - val_loss: 0.9609 - val_categorical_accuracy: 0.5900 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.3211 - categorical_accuracy: 0.8884\n",
            "Epoch 13: saving model to model_init_2023-04-1716_33_09.254485/model-00013-0.32107-0.88839-1.14514-0.56000.h5\n",
            "11/11 [==============================] - 69s 7s/step - loss: 0.3211 - categorical_accuracy: 0.8884 - val_loss: 1.1451 - val_categorical_accuracy: 0.5600 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.2493 - categorical_accuracy: 0.9201\n",
            "Epoch 14: saving model to model_init_2023-04-1716_33_09.254485/model-00014-0.24925-0.92006-0.85216-0.72000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 0.2493 - categorical_accuracy: 0.9201 - val_loss: 0.8522 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1999 - categorical_accuracy: 0.9457\n",
            "Epoch 15: saving model to model_init_2023-04-1716_33_09.254485/model-00015-0.19986-0.94570-0.80445-0.73000.h5\n",
            "11/11 [==============================] - 68s 7s/step - loss: 0.1999 - categorical_accuracy: 0.9457 - val_loss: 0.8044 - val_categorical_accuracy: 0.7300 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1891 - categorical_accuracy: 0.9427\n",
            "Epoch 16: saving model to model_init_2023-04-1716_33_09.254485/model-00016-0.18911-0.94268-0.77654-0.75000.h5\n",
            "11/11 [==============================] - 66s 7s/step - loss: 0.1891 - categorical_accuracy: 0.9427 - val_loss: 0.7765 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1618 - categorical_accuracy: 0.9623\n",
            "Epoch 17: saving model to model_init_2023-04-1716_33_09.254485/model-00017-0.16180-0.96229-0.72166-0.75000.h5\n",
            "11/11 [==============================] - 68s 7s/step - loss: 0.1618 - categorical_accuracy: 0.9623 - val_loss: 0.7217 - val_categorical_accuracy: 0.7500 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1424 - categorical_accuracy: 0.9698\n",
            "Epoch 18: saving model to model_init_2023-04-1716_33_09.254485/model-00018-0.14235-0.96983-0.68695-0.76000.h5\n",
            "11/11 [==============================] - 70s 7s/step - loss: 0.1424 - categorical_accuracy: 0.9698 - val_loss: 0.6870 - val_categorical_accuracy: 0.7600 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1258 - categorical_accuracy: 0.9759\n",
            "Epoch 19: saving model to model_init_2023-04-1716_33_09.254485/model-00019-0.12585-0.97587-0.70695-0.72000.h5\n",
            "11/11 [==============================] - 67s 7s/step - loss: 0.1258 - categorical_accuracy: 0.9759 - val_loss: 0.7070 - val_categorical_accuracy: 0.7200 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.1175 - categorical_accuracy: 0.9759\n",
            "Epoch 20: saving model to model_init_2023-04-1716_33_09.254485/model-00020-0.11749-0.97587-0.76827-0.74000.h5\n",
            "\n",
            "Epoch 20: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
            "11/11 [==============================] - 70s 7s/step - loss: 0.1175 - categorical_accuracy: 0.9759 - val_loss: 0.7683 - val_categorical_accuracy: 0.7400 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f205a965490>"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model 6 : (CONV2D + GRU) no of frames are 12 , image_size = (70,70) , batch_size 64 , no of epochs = 20"
      ],
      "metadata": {
        "id": "t3zbT6OU45_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_idx,img_size,batch_size,num_epochs = global_vars([0,3,6,9,11,13,15,18,20,22,24,27],70,64,20)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SSCk6_q14rO8",
        "outputId": "e92c47af-1567-4c3c-bcc4-fbcc5bdb45a4",
        "trusted": true
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model Features: No of Epochs = 20, batch_size = 64,img_size = 70, no of frames = 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.convolutional import  Conv2D, MaxPooling2D\n",
        "from keras.layers import TimeDistributed,LSTM ,ConvLSTM2D, GRU\n",
        "\n",
        "model = Sequential()    \n",
        "\n",
        "model.add(TimeDistributed(Conv2D(16, (3, 3) , padding='same', activation='relu'),input_shape=(len(img_idx),img_size,img_size,3)))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model.add(TimeDistributed(Conv2D(32, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model.add(TimeDistributed(Conv2D(64, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model.add(TimeDistributed(Conv2D(128, (3, 3) , padding='same', activation='relu')))\n",
        "model.add(TimeDistributed(BatchNormalization()))\n",
        "model.add(TimeDistributed(MaxPooling2D((2, 2))))\n",
        "        \n",
        "model.add(TimeDistributed(Flatten()))\n",
        "\n",
        "model.add(GRU(64))\n",
        "model.add(Dropout(0.25))\n",
        "model.add(Dense(128,activation='relu'))\n",
        "model.add(Dropout(0.25))\n",
        "        \n",
        "model.add(Dense(5, activation='softmax'))"
      ],
      "metadata": {
        "id": "1zwzL8zg4rSd",
        "trusted": true
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimiser = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.7, nesterov=True)\n",
        "model.compile(optimizer=optimiser, loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
      ],
      "metadata": {
        "id": "Z4ATPWSD4rbQ",
        "trusted": true
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4720UiUJ5DFF",
        "outputId": "f7d89bf9-ed9e-43e0-db4b-cd123ad916ca",
        "trusted": true
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " time_distributed_16 (TimeDi  (None, 12, 70, 70, 16)   448       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_17 (TimeDi  (None, 12, 70, 70, 16)   64        \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_18 (TimeDi  (None, 12, 35, 35, 16)   0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_19 (TimeDi  (None, 12, 35, 35, 32)   4640      \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_20 (TimeDi  (None, 12, 35, 35, 32)   128       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_21 (TimeDi  (None, 12, 17, 17, 32)   0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_22 (TimeDi  (None, 12, 17, 17, 64)   18496     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_23 (TimeDi  (None, 12, 17, 17, 64)   256       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_24 (TimeDi  (None, 12, 8, 8, 64)     0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_25 (TimeDi  (None, 12, 8, 8, 128)    73856     \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_26 (TimeDi  (None, 12, 8, 8, 128)    512       \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_27 (TimeDi  (None, 12, 4, 4, 128)    0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " time_distributed_28 (TimeDi  (None, 12, 2048)         0         \n",
            " stributed)                                                      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 64)                405888    \n",
            "                                                                 \n",
            " dropout_12 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               8320      \n",
            "                                                                 \n",
            " dropout_13 (Dropout)        (None, 128)               0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 5)                 645       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 513,253\n",
            "Trainable params: 512,773\n",
            "Non-trainable params: 480\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_generator = generator(train_path, train_doc, batch_size,img_size,img_idx)\n",
        "val_generator = generator(val_path, val_doc, batch_size,img_size,img_idx)\n",
        "\n",
        "if (num_train_sequences%batch_size) == 0:\n",
        "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
        "else:\n",
        "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
        "\n",
        "if (num_val_sequences%batch_size) == 0:\n",
        "    validation_steps = int(num_val_sequences/batch_size)\n",
        "else:\n",
        "    validation_steps = (num_val_sequences//batch_size) + 1\n",
        "\n",
        "print(steps_per_epoch)\n",
        "print(validation_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHa2JPk75DIl",
        "outputId": "48792783-8b4a-4db4-faaf-15b036498b19",
        "trusted": true
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11\n",
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs,verbose=1, \n",
        "                    callbacks=callbacks_list, validation_data=val_generator, \n",
        "                    validation_steps=validation_steps, class_weight=None, workers=1, initial_epoch=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIEicFK15DN4",
        "outputId": "4d4f2819-360e-42a0-e805-ed338f624f1e",
        "trusted": true
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.8007 - categorical_accuracy: 0.1825\n",
            "Epoch 1: saving model to model_init_2023-04-1716_33_09.254485/model-00001-1.80065-0.18250-1.69188-0.14000.h5\n",
            "11/11 [==============================] - 69s 6s/step - loss: 1.8007 - categorical_accuracy: 0.1825 - val_loss: 1.6919 - val_categorical_accuracy: 0.1400 - lr: 0.0010\n",
            "Epoch 2/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.6209 - categorical_accuracy: 0.2519\n",
            "Epoch 2: saving model to model_init_2023-04-1716_33_09.254485/model-00002-1.62089-0.25189-1.67615-0.22000.h5\n",
            "11/11 [==============================] - 66s 7s/step - loss: 1.6209 - categorical_accuracy: 0.2519 - val_loss: 1.6762 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
            "Epoch 3/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.5310 - categorical_accuracy: 0.3167\n",
            "Epoch 3: saving model to model_init_2023-04-1716_33_09.254485/model-00003-1.53100-0.31674-1.66914-0.23000.h5\n",
            "11/11 [==============================] - 62s 6s/step - loss: 1.5310 - categorical_accuracy: 0.3167 - val_loss: 1.6691 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 4/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.4477 - categorical_accuracy: 0.3741\n",
            "Epoch 4: saving model to model_init_2023-04-1716_33_09.254485/model-00004-1.44773-0.37406-1.66073-0.22000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 1.4477 - categorical_accuracy: 0.3741 - val_loss: 1.6607 - val_categorical_accuracy: 0.2200 - lr: 0.0010\n",
            "Epoch 5/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.3881 - categorical_accuracy: 0.4178\n",
            "Epoch 5: saving model to model_init_2023-04-1716_33_09.254485/model-00005-1.38813-0.41780-1.64344-0.23000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 1.3881 - categorical_accuracy: 0.4178 - val_loss: 1.6434 - val_categorical_accuracy: 0.2300 - lr: 0.0010\n",
            "Epoch 6/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.3361 - categorical_accuracy: 0.4585\n",
            "Epoch 6: saving model to model_init_2023-04-1716_33_09.254485/model-00006-1.33607-0.45852-1.67615-0.18000.h5\n",
            "11/11 [==============================] - 65s 6s/step - loss: 1.3361 - categorical_accuracy: 0.4585 - val_loss: 1.6762 - val_categorical_accuracy: 0.1800 - lr: 0.0010\n",
            "Epoch 7/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2869 - categorical_accuracy: 0.4811\n",
            "Epoch 7: saving model to model_init_2023-04-1716_33_09.254485/model-00007-1.28692-0.48115-1.62490-0.27000.h5\n",
            "11/11 [==============================] - 66s 7s/step - loss: 1.2869 - categorical_accuracy: 0.4811 - val_loss: 1.6249 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
            "Epoch 8/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.2362 - categorical_accuracy: 0.5234\n",
            "Epoch 8: saving model to model_init_2023-04-1716_33_09.254485/model-00008-1.23623-0.52338-1.65442-0.26000.h5\n",
            "11/11 [==============================] - 61s 6s/step - loss: 1.2362 - categorical_accuracy: 0.5234 - val_loss: 1.6544 - val_categorical_accuracy: 0.2600 - lr: 0.0010\n",
            "Epoch 9/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.1702 - categorical_accuracy: 0.5505\n",
            "Epoch 9: saving model to model_init_2023-04-1716_33_09.254485/model-00009-1.17016-0.55053-1.57296-0.28000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 1.1702 - categorical_accuracy: 0.5505 - val_loss: 1.5730 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
            "Epoch 10/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.1466 - categorical_accuracy: 0.5762\n",
            "Epoch 10: saving model to model_init_2023-04-1716_33_09.254485/model-00010-1.14661-0.57617-1.58951-0.29000.h5\n",
            "11/11 [==============================] - 60s 6s/step - loss: 1.1466 - categorical_accuracy: 0.5762 - val_loss: 1.5895 - val_categorical_accuracy: 0.2900 - lr: 0.0010\n",
            "Epoch 11/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0647 - categorical_accuracy: 0.6456\n",
            "Epoch 11: saving model to model_init_2023-04-1716_33_09.254485/model-00011-1.06466-0.64555-1.51397-0.28000.h5\n",
            "11/11 [==============================] - 64s 6s/step - loss: 1.0647 - categorical_accuracy: 0.6456 - val_loss: 1.5140 - val_categorical_accuracy: 0.2800 - lr: 0.0010\n",
            "Epoch 12/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0803 - categorical_accuracy: 0.6244\n",
            "Epoch 12: saving model to model_init_2023-04-1716_33_09.254485/model-00012-1.08034-0.62443-1.54455-0.27000.h5\n",
            "11/11 [==============================] - 65s 6s/step - loss: 1.0803 - categorical_accuracy: 0.6244 - val_loss: 1.5445 - val_categorical_accuracy: 0.2700 - lr: 0.0010\n",
            "Epoch 13/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 1.0101 - categorical_accuracy: 0.6712\n",
            "Epoch 13: saving model to model_init_2023-04-1716_33_09.254485/model-00013-1.01013-0.67119-1.44837-0.33000.h5\n",
            "11/11 [==============================] - 66s 7s/step - loss: 1.0101 - categorical_accuracy: 0.6712 - val_loss: 1.4484 - val_categorical_accuracy: 0.3300 - lr: 0.0010\n",
            "Epoch 14/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9905 - categorical_accuracy: 0.6908\n",
            "Epoch 14: saving model to model_init_2023-04-1716_33_09.254485/model-00014-0.99053-0.69080-1.38934-0.40000.h5\n",
            "11/11 [==============================] - 66s 7s/step - loss: 0.9905 - categorical_accuracy: 0.6908 - val_loss: 1.3893 - val_categorical_accuracy: 0.4000 - lr: 0.0010\n",
            "Epoch 15/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9190 - categorical_accuracy: 0.7059\n",
            "Epoch 15: saving model to model_init_2023-04-1716_33_09.254485/model-00015-0.91904-0.70588-1.35576-0.45000.h5\n",
            "11/11 [==============================] - 65s 7s/step - loss: 0.9190 - categorical_accuracy: 0.7059 - val_loss: 1.3558 - val_categorical_accuracy: 0.4500 - lr: 0.0010\n",
            "Epoch 16/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.9088 - categorical_accuracy: 0.7210\n",
            "Epoch 16: saving model to model_init_2023-04-1716_33_09.254485/model-00016-0.90881-0.72097-1.31394-0.51000.h5\n",
            "11/11 [==============================] - 59s 6s/step - loss: 0.9088 - categorical_accuracy: 0.7210 - val_loss: 1.3139 - val_categorical_accuracy: 0.5100 - lr: 0.0010\n",
            "Epoch 17/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8693 - categorical_accuracy: 0.7164\n",
            "Epoch 17: saving model to model_init_2023-04-1716_33_09.254485/model-00017-0.86928-0.71644-1.28957-0.47000.h5\n",
            "11/11 [==============================] - 63s 6s/step - loss: 0.8693 - categorical_accuracy: 0.7164 - val_loss: 1.2896 - val_categorical_accuracy: 0.4700 - lr: 0.0010\n",
            "Epoch 18/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8476 - categorical_accuracy: 0.7451\n",
            "Epoch 18: saving model to model_init_2023-04-1716_33_09.254485/model-00018-0.84763-0.74510-1.24998-0.51000.h5\n",
            "11/11 [==============================] - 65s 7s/step - loss: 0.8476 - categorical_accuracy: 0.7451 - val_loss: 1.2500 - val_categorical_accuracy: 0.5100 - lr: 0.0010\n",
            "Epoch 19/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.8066 - categorical_accuracy: 0.7738\n",
            "Epoch 19: saving model to model_init_2023-04-1716_33_09.254485/model-00019-0.80660-0.77376-1.21983-0.55000.h5\n",
            "11/11 [==============================] - 65s 6s/step - loss: 0.8066 - categorical_accuracy: 0.7738 - val_loss: 1.2198 - val_categorical_accuracy: 0.5500 - lr: 0.0010\n",
            "Epoch 20/20\n",
            "11/11 [==============================] - ETA: 0s - loss: 0.7848 - categorical_accuracy: 0.7768\n",
            "Epoch 20: saving model to model_init_2023-04-1716_33_09.254485/model-00020-0.78481-0.77677-1.13741-0.60000.h5\n",
            "11/11 [==============================] - 59s 6s/step - loss: 0.7848 - categorical_accuracy: 0.7768 - val_loss: 1.1374 - val_categorical_accuracy: 0.6000 - lr: 0.0010\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f204efd3220>"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DvL9vfQiiogR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}